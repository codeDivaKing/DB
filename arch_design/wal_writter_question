; Design WAL writer, single machine, highest throughput, log is durablely written before the call returns to caller.
; ÂíåÂú∞ÈáåÁöÑlog/data writerÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇMulti-threadÔºåÊØè‰∏™threadÈúÄË¶Åpush data to file„ÄÇÊÄé‰πàÊ†∑ÂèØ‰ª•Low latencyÔºåhigh throughputÔºårecover from server crash„ÄÇ
; Multi-threadÔºåÊØè‰∏™threadÈúÄË¶Åpush data to file„ÄÇÊÄé‰πàÊ†∑ÂèØ‰ª•Low latencyÔºåhigh throughputÔºårecover from server crash„ÄÇ
; ÈúÄË¶ÅËÉΩÊîØÊåÅÂ§öÁ∫øÁ®ãÂπ∂ÂèëÂÜôlogÂà∞diskÈáåÔºåÊ≥®ÊÑèÂ§ÑÁêÜrace conditionÔºåÂÜôpsuedo codeÂ∞±Ë°å„ÄÇÂπ∂ÂèëËøô‰∏ÄÂùóÊàë‰πüÊòØ‰∏çÁÜüÊÇâÊâÄ‰ª•‰πüÊ≤°ÂíãÂÆåÊàê
; functionÂ•ΩÂÉèÂæàÁÆÄÂçïÂ∞±ÊòØÁªôinput bytesÁÑ∂ÂêéÂÜôÂà∞fileÈáåÔºålatencyÂ∞ΩÂèØËÉΩ‰ΩéÂπ∂‰∏î‰øùËØÅdurability„ÄÇ ÂÜçÂÖ∑‰ΩìËÆ∞‰∏çÊ∏ÖÊ•ö‰∫Ü
; ÊÑüËßâËÄÉÁÇπÂ∞±ÊòØlevel dbÁöÑWAL batch writeÁöÑÂÆûÁé∞ÔºåÂæàÂ§ö‰∏≠ÊñáËÆ∫ÂùõÈÉΩÂèØ‰ª•ÊâæÂà∞
   test test
ËÆæËÆ°Â¶Ç‰∏ãÁöÑLibraryÔºö
class DataWriter {
    public DataWriter(String filePathOnDisk) {
    }

    public void push(byte[] data) {
    }
}

Áé∞Âú®Êúâ‰∏Ä‰∏™ÂçïÊú∫serverÔºå‰Ω†Ëøô‰∏™LibÂêåÊó∂Ë¢´‰∏äÂçÉ‰∏™Thread‰ΩøÁî®Ôºå‰ªñ‰ª¨‰∏çÈó¥Êñ≠ÁöÑcall ‚Äúpush‚Äù methodÔºåÈÇ£‰πàËøô‰∫õË¢´pushÁöÑbytesÔºåÈúÄË¶Å‰Ω†ÁöÑLib‰ª•appendÁöÑÂΩ¢ÂºèÂÜôÂÖ•disk‰∏äÁöÑfilePath„ÄÇ

‰Ω†Ë¶ÅËÆæËÆ°Âπ∂Â°´ÂÖÖpseudo codeÔºå‰ΩøÂæóÔºö
1. ÊØè‰∏™Thread pushËøõÊù•ÁöÑdataÔºå‰øùËØÅThread-LevelÊúâÂ∫èÊÄß„ÄÇÊØîÂ¶ÇThread_A push d1Ôºåd2ÔºåÂêåÊó∂Thread_B push d3Ôºåd4„ÄÇÈÇ£‰πàÂÜôÂÆåÁöÑfile contentÂèØ‰ª•ÊòØd1_d2_d3_d4ÊàñËÄÖd1_d3_d4_d2ÔºåÂõ†‰∏∫d1ÂøÖÈ°ªÊó©‰∫éd2Ôºåd3ÂøÖÈ°ªÊó©‰∫éd4„ÄÇ
2. high throughputÔºålow latency„ÄÇÊàëÁöÑÁêÜËß£ÊòØÊúÄÂ•ΩÊØè‰∏™Thread call pushÁöÑÊó∂ÂÄôÔºå‰∏çË¶ÅË¢´block/wait„ÄÇ
3. Â±ïÁé∞‰∏Ä‰∏ãÂÖ∑‰Ωìbytes dataÂ¶Ç‰ΩïË¢´ÂÜôÂÖ•disk file„ÄÇ
4. ËÆ®ËÆ∫‰∏ãpersistenceÔºåÂ¶Ç‰ΩïÂú®server crushÊÉÖÂÜµ‰∏ãÂÅörecovery
Perfect ‚Äî you‚Äôre summarizing a **classic system design + concurrency + storage** interview question often inspired by **LevelDB / RocksDB WAL writer** internals.

Let‚Äôs make it crystal clear üëá

---

## üß© What the Interviewer Is Asking

> ‚ÄúDesign a Write-Ahead Log (WAL) writer on a **single machine** that supports **multi-threaded concurrent log writes**, ensures **durability before returning**, and achieves **low latency + high throughput**, while being **recoverable after crash**.‚Äù

---

## üß† What They‚Äôre Testing

They want to see if you understand:

1. **Durability guarantees** ‚Äî the log must be on disk (`fsync`) before the write call returns.
2. **Concurrency control** ‚Äî many threads writing logs concurrently ‚Üí avoid race conditions.
3. **Batching and I/O optimization** ‚Äî reduce `fsync` cost and improve throughput.
4. **Crash recovery** ‚Äî replay logs on restart.
5. **System design tradeoffs** ‚Äî throughput vs latency vs durability.

---

## ‚öôÔ∏è Key Design Challenges

| Problem                        | Naive Approach             | Issue                     |
| ------------------------------ | -------------------------- | ------------------------- |
| Multiple threads want to write | Each thread calls `fsync`  | Too slow                  |
| Sequential log file writes     | Use lock around file       | Bottleneck                |
| Disk flush is expensive        | Flush per record           | High latency              |
| Need durability                | Must sync before returning | Hard to batch efficiently |

---

## üöÄ High-Level Design Idea

Exactly what **LevelDB / RocksDB** do: **WAL Batching + Single Writer Thread**.

---

### 1Ô∏è‚É£ Architecture Overview

```
+-----------------------------+
| Multiple Writer Threads     |
|   (clients)                 |
|      |                      |
|      v                      |
|   WAL Writer Queue  --->  [ Single WAL Writer Thread ]
|                             |    (sequential disk write)
|                             v
|                           Log File (fsync)
+-----------------------------+
```

---

### 2Ô∏è‚É£ How It Works

#### Step 1. Multiple threads submit log entries

Each client thread does:

```python
log_writer.submit(bytes)
```

‚Üí places its log record into a **shared queue** (e.g., `deque` or `BlockingQueue`) with a **future/promise** to signal when done.

---

#### Step 2. Single writer thread batches writes

One dedicated thread:

* Collects all pending log records from the queue.
* Concatenates them into one big buffer.
* Writes them sequentially to disk.
* Calls `fsync()` **once** for the entire batch.
* Marks all futures in the batch as **complete**.

This is called **group commit** ‚Äî LevelDB does this.

---

#### Step 3. Return to caller only after durable write

Each writer thread waits on its **future** until the WAL writer signals fsync is complete.
‚Üí Guarantees durability while batching many threads together.

---

### 3Ô∏è‚É£ Crash Recovery

After restart:

* Read WAL file sequentially.
* Replay all log records to reconstruct in-memory state.

Since we always `fsync()` before ack, all committed logs are durable.

---

## üß∞ Pseudo Code Example (Simplified)

```python
import threading, queue, os

class WALWriter:
    def __init__(self, file_path):
        self.log_file = open(file_path, "ab", buffering=0)
        self.queue = queue.Queue()
        self.lock = threading.Lock()
        self.thread = threading.Thread(target=self._writer_loop, daemon=True)
        self.thread.start()

    def submit(self, data: bytes):
        event = threading.Event()
        self.queue.put((data, event))
        event.wait()   # wait until fsync done
        return True

    def _writer_loop(self):
        while True:
            # batch collect
            batch = []
            try:
                data, event = self.queue.get(timeout=0.01)
                batch.append((data, event))
            except queue.Empty:
                continue

            # drain rest of queue for batching
            while not self.queue.empty():
                batch.append(self.queue.get_nowait())

            # sequential write
            with self.lock:
                for data, _ in batch:
                    self.log_file.write(data)
                self.log_file.flush()
                os.fsync(self.log_file.fileno())

            # notify all threads in batch
            for _, event in batch:
                event.set()
```

---

## ‚öñÔ∏è Design Tradeoffs

| Choice                     | Effect                                       |
| -------------------------- | -------------------------------------------- |
| **Single writer thread**   | Prevents race condition, maintains log order |
| **Batch fsync**            | Amortizes disk I/O cost                      |
| **Blocking submit**        | Guarantees durability before return          |
| **Asynchronous queue**     | Enables concurrency                          |
| **fsync frequency tuning** | Tradeoff between latency and throughput      |

---

## üß† Optional Enhancements

* **Double buffering** to reduce lock time.
* **Group commit timeout** (flush every X ms or Y records).
* **Preallocated log segments** for faster writes.
* **Memory-mapped files** (mmap) for advanced performance.

---

## üß© In Short

‚úÖ **Multi-threaded submit**
‚úÖ **Single-threaded batched write + fsync**
‚úÖ **Durability guarantee before returning**
‚úÖ **Crash recoverable**
‚úÖ **Optimized throughput via batching**

---

Would you like me to extend this into a **system design interview answer format** (with tradeoffs, diagrams, and optional optimizations) ‚Äî like how you‚Äôd explain it aloud in 3‚Äì5 minutes to an interviewer?
